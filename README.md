# Hex-Software_Model-Evaluation-Data-Science-Project
# 🚀 Model Evaluation Project

## 📚 Overview

This project demonstrates how to evaluate the performance of a machine learning model using key metrics and visualization techniques. We use the **Iris dataset** and a **Random Forest Classifier** to showcase these concepts.

## 📝 Project Highlights

1. **Model Evaluation Metrics**:
    - **Accuracy**
    - **Precision**
    - **Recall**
    - **F1-Score**

2. **Visualizations**:
    - **Confusion Matrix** (Heatmap)
    - **Classification Report**

3. **Tools and Libraries**:
    - Python
    - Libraries: `sklearn`, `matplotlib`, `seaborn`, `pandas`, `numpy`

## 📊 Dataset

- **Name**: Iris Dataset (from `sklearn`)
- **Description**: The dataset contains 150 samples of iris flowers categorized into 3 species:
  - Setosa
  - Versicolor
  - Virginica
- **Features**:
  - Sepal Length (cm)
  - Sepal Width (cm)
  - Petal Length (cm)
  - Petal Width (cm)

## 🛠️ Installation

Follow these steps to set up the project locally:

### 1. Clone the Repository

```bash
git clone https://github.com/krishnayadav23/model-evaluation.git
cd model-evaluation
```

### 2. Install Dependencies

Install the required Python libraries using `pip`:

```bash
pip install -r requirements.txt
```

### 3. Run the Script

Execute the Python script to evaluate the model and generate visualizations:

```bash
python model_evaluation.py
```

## 📈 Sample Output

### Metrics:

```
Accuracy: 1.00
Precision: 1.00
Recall: 1.0
F1-Score: 1.00
```

### Confusion Matrix:

![Confusion Matrix](images/confusion_matrix.png)

### Classification Report:

```
              precision    recall  f1-score   support

    setosa       1.00      1.00      1.00        10
    versicolor   1.00      1.00      1.00         8
    virginica    1.00      1.00      1.00        12

    accuracy                          1.00        30
   macro avg      1.00      1.00      1.00        30
weighted avg      1.00      1.00      1.00        30
```

## 🧠 Key Concepts

- **Accuracy**: Measures overall correctness.
- **Precision**: Proportion of predicted positives that are truly positive.
- **Recall**: Proportion of actual positives correctly identified.
- **F1-Score**: Harmonic mean of precision and recall.
- **Confusion Matrix**: Visualizes correct and incorrect predictions.

## 💻 Technologies Used

- **Python 3.x**
- **Scikit-Learn**
- **Matplotlib**
- **Seaborn**

## 🤝 Acknowledgment

Special thanks to **Hexsoftwares** for their support and inspiration! 🙌

## 🔗 Connect with Me

- **LinkedIn**: (https://www.linkedin.com/in/krishna-yadav-38855831b)
- **GitHub**: (https://github.com/Krishnayadav23)

---

Feel free to ⭐ the repository if you found this useful!

#HappyCoding 🖥️✨
